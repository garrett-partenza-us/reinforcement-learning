{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "centered-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hearing-terminology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Four rooms environment class copied and pasted from my ex0 submission\n",
    "class Environment:\n",
    "    \n",
    "    def __init__(self, rows, cols, walls):\n",
    "        \n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.walls = walls\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        self.x_prev = 0\n",
    "        self.y_prev = 0        \n",
    "                \n",
    "    def step(self, action):\n",
    "        self.x_prev=self.x\n",
    "        self.y_prev=self.y\n",
    "        if action == \"LEFT\":\n",
    "            self.y=self.y-1\n",
    "            self.correct()\n",
    "            return self.reward()\n",
    "        elif action == \"RIGHT\":\n",
    "            self.y=self.y+1\n",
    "            self.correct()\n",
    "            return self.reward()\n",
    "        elif action == \"DOWN\":\n",
    "            self.x=self.x+1\n",
    "            self.correct()\n",
    "            return self.reward()\n",
    "        elif action == \"UP\":\n",
    "            self.x=self.x-1\n",
    "            self.correct()\n",
    "            return self.reward()\n",
    "        \n",
    "    def reward(self):\n",
    "        if ((self.x, self.y)) == (self.rows-1, self.cols-1):\n",
    "            return (1, True)\n",
    "        else:\n",
    "            return (0, False)\n",
    "        \n",
    "    def correct(self):\n",
    "        if self.x<0:\n",
    "            self.x=self.x_prev\n",
    "        elif self.x==self.rows:\n",
    "            self.x=self.x_prev\n",
    "        elif self.y<0:\n",
    "            self.y=self.y_prev\n",
    "        elif self.y==self.cols:\n",
    "            self.y=self.y_prev\n",
    "        elif ((self.x, self.y)) in self.walls:\n",
    "            self.x=self.x_prev\n",
    "            self.y=self.y_prev\n",
    "            \n",
    "    def restart(self):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        self.x_prev = 0\n",
    "        self.y_prev = 0\n",
    "        \n",
    "    def loc(self):\n",
    "        return (self.x, self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "three-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Hyperparameters\n",
    "action_space = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
    "rows = 11\n",
    "cols = 11\n",
    "# walls = [\n",
    "#     (5, 0),\n",
    "#     (5, 2),\n",
    "#     (5, 3),\n",
    "#     (5, 4),\n",
    "#     (5, 5),\n",
    "#     (4, 5),\n",
    "#     (4, 6),\n",
    "#     (4, 7),\n",
    "#     (4, 9),\n",
    "#     (4, 10),\n",
    "#     (0, 5),\n",
    "#     (2, 5),\n",
    "#     (3, 5),\n",
    "#     (6, 5),\n",
    "#     (7, 5),\n",
    "#     (9, 5),\n",
    "#     (10, 5),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "according-custody",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'walls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-48cb907bb2f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menvironment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwalls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'walls' is not defined"
     ]
    }
   ],
   "source": [
    "environment = Environment(rows, cols, walls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition():\n",
    "    \n",
    "    def __init__(self, state, action, state_new, reward, term):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.state_new = state_new\n",
    "        self.reward = reward\n",
    "        self.term = term\n",
    "        \n",
    "class ReplayMemory():\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dqn model clas \n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 2, 1)\n",
    "        self.lin1 = nn.Linear(64,32)\n",
    "        self.lin2 = nn.Linear(32,4)\n",
    "        self.drop = nn.Dropout(0.4)\n",
    "\n",
    "    def forward(self, x, batch_size):\n",
    "        \n",
    "        x = x.reshape(batch_size, 2, 1)\n",
    "        board_states = []\n",
    "        for state in x:\n",
    "            board = np.zeros((rows,cols))\n",
    "            board[int(state[0])][int(state[1])] = 1\n",
    "            board_states.append(board)\n",
    "\n",
    "        x = torch.tensor(board_states).float()\n",
    "        x = x.reshape(batch_size, 1, rows, cols)\n",
    "        x = nn.ReLU()(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = nn.ReLU()(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.drop(x)\n",
    "        x = x.reshape(batch_size,-1)\n",
    "        x = nn.ReLU()(self.lin1(x))\n",
    "        x = self.lin2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(net, state, epsilon, batch_size):\n",
    "    greedy = np.random.choice([True, False], p=[1-epsilon, epsilon])\n",
    "    if greedy:\n",
    "        action = torch.argmax(net(torch.tensor(state).float(), batch_size)[0], dim=0).item()\n",
    "    else:\n",
    "        action = random.choice([0,1,2,3])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-string",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        #hyperparameters\n",
    "        self.exp_replay_size = 100000\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 0.1\n",
    "        self.num_episodes = 500\n",
    "        self.batch_size = 64\n",
    "        self.steps = 0\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.max_moves_per_game = 1000\n",
    "        self.lr = 1e-4\n",
    "        self.eps_decay = 5e-7\n",
    "        \n",
    "        #networks\n",
    "        self.QNet = DQN()\n",
    "        self.TNet = DQN()\n",
    "        self.optimizer = torch.optim.Adam(lr=self.lr, params=self.QNet.parameters())\n",
    "        \n",
    "        #replay buffer\n",
    "        self.ER = ReplayMemory(self.exp_replay_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(agent):\n",
    "    agent.QNet.train()\n",
    "    sample_transitions = agent.ER.sample(agent.batch_size)\n",
    "    \n",
    "    if agent.steps%1000==0 and len(agent.ER)>=agent.batch_size:\n",
    "        agent.TNet.load_state_dict(agent.QNet.state_dict())\n",
    "\n",
    "    #calculate q-values and expected values\n",
    "    qp = agent.QNet(torch.tensor([transition.state for transition in sample_transitions]).float(), agent.batch_size)\n",
    "    #get index for the choosen action \n",
    "    qa = [transition.action for transition in sample_transitions]\n",
    "    qp = torch.tensor([a[idx] for a, idx in list(zip(qp,qa))])\n",
    "    #get expected returns using target network \n",
    "    qn = agent.TNet(torch.tensor([transition.state_new for transition in sample_transitions]).float(), agent.batch_size)\n",
    "    qn = torch.max(qn, dim=1).values\n",
    "    qn = qn + torch.tensor([transition.reward for transition in sample_transitions])\n",
    "    qn = agent.gamma * qn\n",
    "    #get terminated boolean values\n",
    "    term_flags = [transition.term for transition in sample_transitions]\n",
    "    for idx, flag in enumerate(term_flags):\n",
    "        if flag:\n",
    "            qn[idx]=0\n",
    "    #decay exploration\n",
    "#     agent.epsilon = max(0, agent.epsilon-agent.eps_decay)\n",
    "    return agent.loss_func(qp, qn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-cleveland",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()\n",
    "losses = []\n",
    "steps2finish = []\n",
    "\n",
    "for episode in tqdm(range(agent.num_episodes)):\n",
    "    \n",
    "    environment = Environment(rows, cols, walls)\n",
    "    moves = 0\n",
    "\n",
    "    while moves < agent.max_moves_per_game:\n",
    "        \n",
    "        agent.QNet.eval()\n",
    "        #observation\n",
    "        state = environment.loc()\n",
    "        action = get_action(agent.QNet, state, agent.epsilon, 1)\n",
    "        reward, terminated = environment.step(action_space[action])\n",
    "        state_new = environment.loc()\n",
    "        #append to replay buffer\n",
    "        agent.ER.push(state, action, state_new, reward, terminated)\n",
    "        moves +=1\n",
    "            \n",
    "        #increament step count\n",
    "        agent.steps+=1\n",
    "        if len(agent.ER)>=agent.batch_size and agent.steps%256==0:\n",
    "            agent.optimizer.zero_grad()\n",
    "            loss = optimize(agent)\n",
    "            loss.backward()\n",
    "            agent.optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        #break when episode is complete\n",
    "        if terminated:\n",
    "            break\n",
    "                \n",
    "    steps2finish.append(moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Loss Curve')\n",
    "plt.plot(np.convolve(losses, np.ones(10), 'valid') / 10, label='Rolling MSE Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Moves per Epsiode')\n",
    "plt.plot(np.convolve(steps2finish, np.ones(10), 'valid') / 10, label='Rolling Steps Average')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-korea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
