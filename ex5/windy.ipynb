{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fancy-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "synthetic-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps numeric actions to semantic actions\n",
    "action_map = {\n",
    "    1: \"UP\",\n",
    "    2: \"DOWN\",\n",
    "    3: \"LEFT\",\n",
    "    4: \"RIGHT\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "indian-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input q-function --> Output value function\n",
    "# Used for plotting value grids with imshow\n",
    "def get_value_function(q):\n",
    "    v = {}\n",
    "    for row in range(7):\n",
    "        for col in range(10):\n",
    "            v[tuple((row,col))] = max([\n",
    "                q[tuple((row,col,1))],\n",
    "                q[tuple((row,col,2))], \n",
    "                q[tuple((row,col,3))], \n",
    "                q[tuple((row,col,4))], \n",
    "            ])\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "genetic-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridWorld Class\n",
    "class GridWorld:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        # Rewards\n",
    "        self.world = np.array([\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ])\n",
    "        # Wind\n",
    "        self.wind = np.array([\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "        ])\n",
    "\n",
    "    # Input x, y, and numeric action --> Output new unclipped x, y\n",
    "    def step(self, x, y, action):\n",
    "        wind = self.wind[x][y]\n",
    "        if action == 1:\n",
    "            return x-1, y\n",
    "        elif action == 2:\n",
    "            return x+1, y\n",
    "        elif action == 3:\n",
    "            return x, y-1\n",
    "        elif action == 4:\n",
    "            return x, y+1\n",
    "        \n",
    "    # Input x, y --> Output reward\n",
    "    def reward(self, x, y):\n",
    "        if x not in range(0,7) or y not in range(0,10):\n",
    "            return -1\n",
    "        else:\n",
    "            return self.world[x][y]\n",
    "        \n",
    "    # Input x, y --> Output True if position is terminal state else False\n",
    "    def terminated(self, x, y):\n",
    "        return True if (x==3 and y==7) else False\n",
    "    \n",
    "    # Input policy --> Output episode in format [(state, action reward)...n]\n",
    "    def episode(self, policy, x=3, y=0, max_steps=1000, epsilon=0.1):\n",
    "        episode = []\n",
    "        for step in range(max_steps):\n",
    "            if np.random.choice([True, False], p=[epsilon, 1-epsilon]):\n",
    "                action = random.choice(list(action_map.keys()))\n",
    "            else:\n",
    "                action = policy[tuple([x, y])]\n",
    "            x_new, y_new = self.step(x, y, action)\n",
    "            reward = self.reward(x_new, y_new)\n",
    "            x_new = np.clip(x_new, 0, 6)\n",
    "            y_new = np.clip(y_new, 0, 9)\n",
    "            episode.append(tuple([tuple([x, y]), action, reward]))\n",
    "            if self.terminated(x_new, y_new):\n",
    "                break\n",
    "            else:\n",
    "                x=x_new\n",
    "                y=y_new\n",
    "        return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fuzzy-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent class\n",
    "class Agent:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        self.gridworld = GridWorld()\n",
    "        self.x = 3\n",
    "        self.y = 0\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 0.1\n",
    "        self.q = {tuple([row, col, action]): np.random.normal(0, 1) for row in range(self.gridworld.world.shape[0]) for col in range(self.gridworld.world.shape[1]) for action in action_map.keys()}\n",
    "        self.p = {tuple([row, col]): max(action_map.keys(), key = lambda a: self.q[tuple([row, col, a])]) for row in range(self.gridworld.world.shape[0]) for col in range(self.gridworld.world.shape[1])}\n",
    "        self.r = {tuple([row, col, action]): [] for row in range(self.gridworld.world.shape[0]) for col in range(self.gridworld.world.shape[1]) for action in action_map.keys()}\n",
    "    \n",
    "    # Wrapper function for GridWorld.step()\n",
    "    def step(self, action):\n",
    "        self.x, self.y = self.gridworld.step(self.x, self.y, action)\n",
    "        return self.gridworld.reward(self.x, self.y)\n",
    "    \n",
    "    # Wrapper function for GridWorld.episode()\n",
    "    def episode(self):\n",
    "        return self.gridworld.episode(self.p)\n",
    "    \n",
    "    # Monte Carlo on-policy policy evaluation for e-soft policies\n",
    "    def mc_onpolicy(self, iterations):\n",
    "        for iteration in tqdm(range(iterations)):\n",
    "            episode = self.episode()\n",
    "            G = 0\n",
    "            for T, step in enumerate(reversed(episode)):\n",
    "                state, action, reward = step\n",
    "                G = agent.gamma * G + reward\n",
    "                if not any([(state, action) == row for row in [(x[0], x[1]) for x in episode[:-T-1]]]):\n",
    "                    sa = tuple([state[0], state[1], action])\n",
    "                    self.r[sa].append(G)\n",
    "                    self.q[sa] = np.mean(self.r[sa])\n",
    "                    self.p[state] = max(action_map.keys(), key = lambda a: self.q[tuple([state[0], state[1], a])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "offensive-adelaide",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/10000 [00:02<22:59,  7.24it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-028832c54f89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmc_onpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_value_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-e21f83257c05>\u001b[0m in \u001b[0;36mmc_onpolicy\u001b[0;34m(self, iterations)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmc_onpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mepisode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-e21f83257c05>\u001b[0m in \u001b[0;36mepisode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Wrapper function for GridWorld.episode()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgridworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Monte Carlo on-policy policy evaluation for e-soft policies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-02438db20e2a>\u001b[0m in \u001b[0;36mepisode\u001b[0;34m(self, policy, x, y, max_steps, epsilon)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mx_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0my_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mepisode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "\n",
    "agent.mc_onpolicy(10000)\n",
    "\n",
    "v = get_value_function(agent.q)\n",
    "\n",
    "plt.imshow(np.array([*v.values()]).reshape(7,10))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-reward",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-edgar",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
