{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "phantom-bunny",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "authentic-guide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps numeric actions to semantic actions\n",
    "action_map = {\n",
    "    1: \"UP\",\n",
    "    2: \"DOWN\",\n",
    "    3: \"LEFT\",\n",
    "    4: \"RIGHT\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "useful-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input q-function --> Output value function\n",
    "# Used for plotting value grids with imshow\n",
    "def plot_valuemap(q):\n",
    "    v = {}\n",
    "    for row in range(7):\n",
    "        for col in range(10):\n",
    "            v[tuple((row,col))] = max([\n",
    "                q[tuple((row,col,1))],\n",
    "                q[tuple((row,col,2))], \n",
    "                q[tuple((row,col,3))], \n",
    "                q[tuple((row,col,4))], \n",
    "            ])\n",
    "    plt.imshow(np.array([*v.values()]).reshape(7,10))\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "exact-bloom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridWorld Class\n",
    "class GridWorld:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        # Rewards\n",
    "        self.world = np.array([\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ])\n",
    "        # Wind\n",
    "        self.wind = np.array([\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "        ])\n",
    "\n",
    "    # Input x, y, and numeric action --> Output new unclipped x, y\n",
    "    def step(self, x, y, action):\n",
    "        wind = self.wind[x][y]\n",
    "        if action == 1:\n",
    "            return x-1, y\n",
    "        elif action == 2:\n",
    "            return x+1, y\n",
    "        elif action == 3:\n",
    "            return x, y-1\n",
    "        elif action == 4:\n",
    "            return x, y+1\n",
    "        \n",
    "    # Input x, y --> Output reward\n",
    "    def reward(self, x, y):\n",
    "        if x not in range(0,7) or y not in range(0,10):\n",
    "            return -1\n",
    "        else:\n",
    "            return self.world[x][y]\n",
    "        \n",
    "    # Input x, y --> Output True if position is terminal state else False\n",
    "    def terminated(self, x, y):\n",
    "        return True if (x==3 and y==7) else False\n",
    "    \n",
    "    # Input policy --> Output episode in format [(state, action reward)...n]\n",
    "    def episode(self, policy, x=3, y=0, max_steps=1000, epsilon=0.1):\n",
    "        episode = []\n",
    "        for step in range(max_steps):\n",
    "            if np.random.choice([True, False], p=[epsilon, 1-epsilon]):\n",
    "                action = random.choice(list(action_map.keys()))\n",
    "            else:\n",
    "                action = policy[tuple([x, y])]\n",
    "            x_new, y_new = self.step(x, y, action)\n",
    "            reward = self.reward(x_new, y_new)\n",
    "            x_new = np.clip(x_new, 0, 6)\n",
    "            y_new = np.clip(y_new, 0, 9)\n",
    "            episode.append(tuple([tuple([x, y]), action, reward]))\n",
    "            if self.terminated(x_new, y_new):\n",
    "                break\n",
    "            else:\n",
    "                x=x_new\n",
    "                y=y_new\n",
    "        return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "theoretical-jewel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent class\n",
    "class Agent:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        self.gridworld = GridWorld()\n",
    "        self.x = 3\n",
    "        self.y = 0\n",
    "        self.x_prev = None\n",
    "        self.y_prev = None\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 0.1\n",
    "        self.q = {tuple([row, col, action]): np.random.normal(0, 1) for row in range(self.gridworld.world.shape[0]) for col in range(self.gridworld.world.shape[1]) for action in action_map.keys()}\n",
    "        self.p = {tuple([row, col]): max(action_map.keys(), key = lambda a: self.q[tuple([row, col, a])]) for row in range(self.gridworld.world.shape[0]) for col in range(self.gridworld.world.shape[1])}\n",
    "        self.r = {tuple([row, col, action]): [] for row in range(self.gridworld.world.shape[0]) for col in range(self.gridworld.world.shape[1]) for action in action_map.keys()}\n",
    "    \n",
    "    # Wrapper function for GridWorld.step()\n",
    "    def step(self, action):\n",
    "        self.x_prev = self.x\n",
    "        self.y_prev = self.y\n",
    "        self.x, self.y = self.gridworld.step(self.x, self.y, action)\n",
    "        reward = self.gridworld.reward(self.x, self.y)\n",
    "        self.x = np.clip(self.x, 0, 6)\n",
    "        self.y = np.clip(self.y, 0, 9)\n",
    "        return reward\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x = 3\n",
    "        self.y = 0\n",
    "    \n",
    "    # Wrapper function for GridWorld.episode()\n",
    "    def episode(self):\n",
    "        return self.gridworld.episode(self.p)\n",
    "    \n",
    "    # Wrapper function for GridWorld.reward()\n",
    "    def reward(self):\n",
    "        return self.gridworld.reward(self.x, self.y)\n",
    "    \n",
    "    # Wrapper function for GridWorld.terminated()\n",
    "    def terminated(self):\n",
    "        return self.gridworld.terminated(self.x, self.y)\n",
    "    \n",
    "    # Monte Carlo control on-policy policy evaluation (for e-soft policies)\n",
    "    def mc_onpolicy(self, iterations):\n",
    "        for iteration in tqdm(range(iterations)):\n",
    "            episode = self.episode()\n",
    "            G = 0\n",
    "            for T, step in enumerate(reversed(episode)):\n",
    "                state, action, reward = step\n",
    "                G = agent.gamma * G + reward\n",
    "                if not any([(state, action) == row for row in [(x[0], x[1]) for x in episode[:-T-1]]]):\n",
    "                    sa = tuple([state[0], state[1], action])\n",
    "                    self.r[sa].append(G)\n",
    "                    self.q[sa] = np.mean(self.r[sa])\n",
    "                    self.p[state] = max(action_map.keys(), key = lambda a: self.q[tuple([state[0], state[1], a])])\n",
    "                    \n",
    "    # SARSA on-policy (TD control)\n",
    "    def SARSA(self, iterations):\n",
    "        alpha = 0.5\n",
    "        epsilon = 0.1\n",
    "        max_steps = 1000\n",
    "        self.q[tuple([3,7,1])] = 0\n",
    "        self.q[tuple([3,7,2])] = 0\n",
    "        self.q[tuple([3,7,3])] = 0\n",
    "        self.q[tuple([3,7,4])] = 0\n",
    "        for iteration in tqdm(range(iterations)):\n",
    "            self.reset()\n",
    "            for step in range(max_steps):\n",
    "                if np.random.choice([True, False], p=[epsilon, 1-epsilon]):\n",
    "                    A1 = random.choice(list(action_map.keys()))\n",
    "                else:\n",
    "                    A1 = max(action_map.keys(), key = lambda a: self.q[tuple([self.x, self.y, a])])\n",
    "                reward = self.step(A1)\n",
    "                if np.random.choice([True, False], p=[epsilon, 1-epsilon]):\n",
    "                    A2 = random.choice(list(action_map.keys()))\n",
    "                else:\n",
    "                    A2 = max(action_map.keys(), key = lambda a: self.q[tuple([self.x, self.y, a])])\n",
    "                sa1 = tuple([self.x_prev, self.y_prev, A1])\n",
    "                sa2 = tuple([self.x, self.y, A2])\n",
    "                agent.q[sa1] = agent.q[sa1] + alpha * ( reward + self.gamma * agent.q[sa2] - agent.q[sa1])\n",
    "                if self.terminated():\n",
    "                    break\n",
    "                    \n",
    "    def qLearning(self, iterations):\n",
    "        pass\n",
    "    \n",
    "    def expSARSA(self, iterations):\n",
    "        pass\n",
    "    \n",
    "    def nstepSARSA(self, iterations):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "banned-salvation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:10<00:00, 917.31it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAADtCAYAAAD+6b0PAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAUpklEQVR4nO3df5BdZWHG8e/D5icBAhJQTKKgRiqlKswOP0zHKqAN6EBnap3gYKvDmH+EolIdbDvoUP3D2lrtlKFuEW0RpTZiJ2OjQSsMQyuUQBgkgdRMtGRjNPwIEIIhye7TP+4NXvJj79nNOefec/N8Zs54z92z7/tewz777nve9z2yTUREVOeIXjcgImLQJWgjIiqWoI2IqFiCNiKiYgnaiIiKJWgjIiqWoI2IaJN0k6Stkh4+yNcl6e8lbZD0kKQzi5SboI2I+I2vAUsm+PqFwKL2sQy4oUihCdqIiDbbdwFPTXDJJcC/uOUe4FhJJ3Urd1pZDYyI6IXff/scP/nUWKFr73/ohbXAzo63RmyPTKK6+cCmjvPR9ntbJvqmBG1ENNqTT43xP6teVejaoZN+utP2cMVN2k+CNiIazcA443VVtxlY2HG+oP3ehDJGGxGNZsxujxU6SrAC+OP27INzgGdsTzhsAOnRRsQAKKtHK+mbwNuAeZJGgU8B0wFs/yOwErgI2AA8D3ywSLkJ2ohoNGPGStru1falXb5u4MOTLTdBGxGNN05/76udoI2IRjMwlqCNiKiOgd2ubdbBlCRoI6Lx+jtmE7QR0XDGGTqIiKiUYay/czZBGxHN1loZ1t8StBHRcGIM9boRE0rQRkSjtWYdHIZBO2NotmdPm1tF0fsbL2X9cjE1jQPtftmseioCpu2sb3BLu+v7txqfOVRbXbuPrOeH3DV2i2Y+Vc8f4ztfeJpdu3cc0v+BrXm0h2HQzp42l7csuKyKovfjZ7fXUg8A4/WE0pb3vqGWegCOX7ez+0UlmfGr+v6tnn/NsbXVtfXM6bXUs/OE+n5RLfr687XUc+/DXy6lnPHDsUcbEVGXw7ZHGxFRFyPG+nzH1wRtRDRehg4iIipkxC7Xd/NzKhK0EdForQULGTqIiKhUboZFRFTIFmNOjzYiolLj6dFGRFSndTOsv6OsUH9b0hJJ6yVtkHRN1Y2KiChq782wIkevdP01IGkIuB54BzAK3Cdphe11VTcuIqKIsQGYR3sWsMH2RgBJtwKXAAnaiOi5QVkZNh/Y1HE+Cpy970WSlgHLAGZNO7qUxkVEFDF+uMw6sD0CjADMnfmKPn+wREQMitamMs0P2s3Awo7zBe33IiJ6zojdA7AE9z5gkaRTaAXsUuB9lbYqIqIgm+YvWLC9R9IVwCpgCLjJ9trKWxYRUYgGY8GC7ZXAyorbEhExaWYAerQREf1uEG6GRUT0LaNs/B0RUaXW48b7O8r6u78dEdGVGCt4FCqty94ukl4l6Q5JayQ9JOmibmX296+BiIguTHkrwwru7fKXwLds3yDpNFoTBU6eqNz0aCOi8Urs0b64t4vtXcDevV06GTim/Xou8ItuhaZHGxGNZmsyPdp5klZ3nI+0tw/Yq8jeLp8Gbpd0JTAHuKBbpZUE7e6509my5JVVFL2fX59Y393GF142Xks9PmpXLfUAbH/tjNrq4hVH1VbV0t++p7a6fnz1WbXUM7psdy31AMz8/OO11HPEhw79M7VuhhVegvuE7eFDrPJS4Gu2/1bSucDNkk63fdCASI82Ihqu1GeGFdnb5XJgCYDtH0uaBcwDth6s0IzRRkSjtW6GqdBRwIt7u0iaQWtvlxX7XPMYcD6ApDcAs4AJ/wRIjzYiGq+slWEH29tF0nXAatsrgKuBf5L0UVo5/wHbE24Nm6CNiEYre2XYgfZ2sX1tx+t1wOLJlJmgjYjG6+WDF4tI0EZEo9mwezxBGxFRmdbQQYI2IqJSRfcx6JUEbUQ02t7pXf2sa39b0k2Stkp6uI4GRURMTmvooMjRK0Vq/hrtVRAREf3Ght0+otDRK0UezniXpJOrb0pExNQcNjfDJC0DlgFMP+q4soqNiJhQEx5lU9qvAdsjtodtD0+bPaesYiMiuhpvP3K829ErmXUQEY3WhFkHCdqIaLx+H6MtMr3rm8CPgVMljUq6vPpmRUQUY4s9PqLQ0StFZh1cWkdDIiKmKkMHEREVyhhtREQNErQRERVqwjzaBG1ENF4v58gWkaCNiEazYU82/o6IqFaGDiIiKpQx2oiIGvhwDNqxGbD91VWUfIC6jh6rpyJAx+yqpZ7jjt1RSz0Arz31ydrqeuMxm2ur6+1Hrautrs/c/JNa6rlrZy3VAHD3c6fWUs+aoXJ+pnIzLCKiQjaM5WZYRESVMkYbEVG5w3KMNiKiLtnrICKiam6N0/azBG1ENF5mHUREVMgosw4iIqqWoYOIiIr1+6yDIs8MWyjpDknrJK2VdFUdDYuIKMJuBW2RowhJSyStl7RB0jUHuea9HZn4jW5lFunR7gGutv2ApKOB+yX9wHZ9axwjIiZQ1vQuSUPA9cA7gFHgPkkrOvNO0iLgk8Bi29skndit3K49WttbbD/Qfr0deASYP7WPERFRPrvYUcBZwAbbG23vAm4FLtnnmg8B19ve1qrbW7sVOqlbdZJOBs4A7j3A15ZJWi1p9fiO+jZFiYjDmxHj40cUOoB5e3OqfSzbp7j5wKaO81H271i+Hni9pP+SdI+kJd3aWPhmmKSjgG8DH7H97H4f1h4BRgBmLljY5/cAI2KQTCJwnrA9fIjVTQMWAW8DFgB3Sfod208f7BsK9WglTacVsrfYvu0QGxkRUZ5yb4ZtBhZ2nC9ov9dpFFhhe7ftnwH/Syt4D6rIrAMBXwEesf2FIi2NiKiVCx7d3QcsknSKpBnAUmDFPtf8O63eLJLm0RpK2DhRoUV6tIuB9wPnSXqwfVxUqMkRETUoq0drew9wBbCK1o3/b9leK+k6SRe3L1sFPClpHXAH8HHbE+6g33WM1vbd0OcLiSPisGVgfLy8iLK9Eli5z3vXdrw28LH2UUhWhkVEsxno85VhCdqIaLzsdRARUbUEbURElYrvY9ArCdqIaL70aCMiKmRwibMOqpCgjYgBkKCNiKjWYTl0cASMza7nk3v2WC31AJx4/PZa6nndsU/UUg/Am47Z1P2ikrzlyJ/WVtfiWfU9Q+qiM95ZSz0r19xeSz0t62up5eahneUUdFgGbUREXbJgISKielmwEBFRtcw6iIioltKjjYioUPG9ZnsmQRsRDafcDIuIqFx6tBERFRvvdQMmlqCNiGYbhHm0kmYBdwEz29cvt/2pqhsWEVHUIMw6eAE4z/Zz7ceO3y3pe7bvqbhtERHFND1o2w8ie659Or199PnHiojoH4V23pA0JOlBYCvwA9v3HuCaZZJWS1o99tyOstsZEXFQcrGjVwoFre0x228GFgBnSTr9ANeM2B62PTx01Jyy2xkRcWCmtQS3yNEjk9pLzvbTwB3AkmqaExExBS549EjXoJV0gqRj269nA+8AHq26YRERRfX70EGRWQcnAf8saYhWMH/L9nerbVZExCT0+e35IrMOHgLOqKEtERFT0/SgjYjoZzIo+9FGRFQsPdqIiGoNwhLciIj+1udBW98zmSMiqlBwalfRXq+kJZLWS9og6ZoJrvtDSZY03K3MBG1ENF9JCxba01ivBy4ETgMulXTaAa47GrgK2G87ggNJ0EZE42m82FHAWcAG2xtt7wJuBS45wHV/BXwO2Fmk0GrGaMdh6Nf1TLfYM72+3xVPPXtkLfWs2/PyWuoB2DlW3zD97vH66hpjfW11rVxzey313FXoR7oct23r+tdwKZ7e82Qt9UzCfGBTx/kocHbnBZLOBBba/g9JHy9SaG6GRUTzFb8ZNk/S6o7zEdsjRb9Z0hHAF4APFK6RBG1ENN3k9jF4wvZE3fXNwMKO8wXt9/Y6GjgduFMSwCuAFZIutt0Z4C+RoI2I5itvetd9wCJJp9AK2KXA+16sxn4GmLf3XNKdwJ9NFLKQm2ERMQhKmnVgew9wBbAKeITWJlprJV0n6eKpNi892ohoNFF4RkEhtlcCK/d579qDXPu2ImUmaCOi2Xq812wRCdqIaL4EbURExRK0ERHVytBBRESVDJR4M6wKhad3SRqStEZSnhcWEX2l3x/OOJl5tFfRmlcWEdFfmv64cQBJC4B3ATdW25yIiMkblB7tF4FPMMFIiKRlklZLWj2+Y0cpjYuIKKTpPVpJ7wa22r5/outsj9getj18xJw5pTUwImJCRUO2h0FbZNbBYuBiSRcBs4BjJH3d9mXVNi0iojvR/9O7uvZobX/S9gLbJ9PayeZHCdmI6Cf9PkabebQR0Xx93qOdVNDavhO4s5KWRERM1SAFbURE38nuXRERNUjQRkRUq8yNv6uQoI2IxsvQQURElXq8GKGIBG1ENF+CNiKiOk1YGVZJ0Gocpj+nKorevy4P1VIPwNiuI2up55nj6/tM68fqq2vLjmNqq2tofn13Rz77s1NrqWfTtmNrqQfgjSf9opZ6dpf086vx/k7a9GgjotkyRhsRUb3DcuggIqJWCdqIiGqlRxsRUbUEbUREhZwluBERlTps59FGRNTK/Z20CdqIaLyB6NFK+jmwHRgD9tgerrJRERGFNWDBQteHM3Z4u+03J2Qjot9ovNhRqCxpiaT1kjZIuuYAX/+YpHWSHpL0n5Je3a3MyQRtRERfKitoJQ0B1wMXAqcBl0o6bZ/L1gDDtt8ILAf+ulu5RYPWwO2S7pe0rOD3RERUz7RuhhU5ujsL2GB7o+1dwK3AJS+pzr7D9vPt03uABd0KLXoz7Hdtb5Z0IvADSY/avqvzgnYALwOYNve4gsVGRBy6SdwMmydpdcf5iO2RjvP5wKaO81Hg7AnKuxz4XrdKCwWt7c3t/90q6Tu0Uv+ufa4ZAUYAZr1yYZ8PTUfEQCmeOE+UdZ9J0mXAMPB73a7tOnQgaY6ko/e+Bt4JPHyojYyIKMPeBQtFjgI2Aws7zhe033tpndIFwF8AF9t+oVuhRXq0Lwe+I2nv9d+w/f0iLY6IqJxd5sbf9wGLJJ1CK2CXAu/rvEDSGcCXgSW2txYptGvQ2t4IvGnSzY2IqEtJOWt7j6QrgFXAEHCT7bWSrgNW214BfB44Cvi3dgf0MdsXT1RuVoZFROOVuTLM9kpg5T7vXdvx+oLJlpmgjYhmM5BnhkVEVKy/czZBGxHNNxCbykRE9LM8bjwiokoN2L0rQRsRjdZasNDfSZugjYjmOxyfGTZjyw4Wfua/qyh6P1785lrqARg978ha6pn22Oxa6gHY/vrptdW167j6fq9/9dlzaqvrNVf+qpZ6Trrl2VrqAThx5vZa6plW0lMV06ONiKhSxmgjIqpW6l4HlUjQRkTzZeggIqJCLv48sF5J0EZE86VHGxFRsf7O2QRtRDSfxvt77CBBGxHNZg7PBQsREXUR7vsFC10fzggg6VhJyyU9KukRSedW3bCIiMLsYkePFO3Rfgn4vu33SJoB1LMWNSKiiD7v0XYNWklzgbcCHwCwvQvYVW2zIiIKasAYbZGhg1OAx4GvSloj6UZJc/a9SNIySaslrd5N18ecR0SURuPjhY5eKRK004AzgRtsnwHsAK7Z9yLbI7aHbQ9PZ2bJzYyIOJiC47M9HF4oErSjwKjte9vny2kFb0RE75nmB63tXwKbJJ3afut8YF2lrYqImIzxgkePFJ11cCVwS3vGwUbgg9U1KSJicvp9Hm2hoLX9IDBccVsiIibPwFh/TzvIyrCIaLjejr8WkaCNiOZL0EZEVCxBGxFRIQN5ZlhERJUMzs2wiIjqNGDWQaFtEiMi+lqJK8MkLZG0XtIGSfttNyBppqR/bX/9XkkndyszQRsRzVdS0EoaAq4HLgROAy6VdNo+l10ObLP9OuDvgM91K7eSoYPtbHvih17+f5P8tnnAE5Ou7O7lk/6WKbt7St81tc/V3wbxM8EUP9dPK2jIAZ0/pe+a0me6c0pVTcmrD72IUufRngVssL0RQNKtwCW8dNuBS4BPt18vB/5BkuyDN6KSoLV9wmS/R9Jq2wO3+mwQP9cgfiYYzM81iJ9pPwaKb4E4T9LqjvMR2yMd5/OBTR3no8DZ+5Tx4jW290h6BjieCX6h5WZYRDRf8R7tE734xZOgjYiGc5mzDjYDCzvOF7TfO9A1o5KmAXOBJycqtJ9uho10v6SRBvFzDeJngsH8XIP4mV7KYI8XOgq4D1gk6ZT2boVLgRX7XLMC+JP26/cAP5pofBZAXb4eEdHX5k47wece8weFrl217cb7uw0dSLoI+CIwBNxk+7OSrgNW214haRZwM3AG8BSwdO/Ns4PJ0EFENF+JHUbbK4GV+7x3bcfrncAfTabMBG1ENJs9mVkHPdHzMdpuqzCaSNJCSXdIWidpraSret2mskgaaj8N+bu9bktZJB0rabmkRyU9IuncXrepDJI+2v7v72FJ32z/yTuQPDZW6OiVngZtwVUYTbQHuNr2acA5wIcH5HMBXAU80utGlOxLwPdt/xbwJgbg80maD/wpMGz7dFrjjUt726qqDMZTcKv04ioM27uAvaswGs32FtsPtF9vp/WDO7+3rTp0khYA7wJu7HVbyiJpLvBW4CsAtnfZfrq3rSrNNGB2ewrSkcAvetyeauzdJrHI0SO9DtoDrcJofCB1am84cQZw78RXNsIXgU/Q0+eJlu4U4HHgq+0hkRslzel1ow6V7c3A3wCPAVuAZ2zf3ttWVcjjxY4e6XXQDjRJRwHfBj5i+9let+dQSHo3sNX2/b1uS8mmAWcCN9g+A9gBNP5egaTjaP11eArwSmCOpMt626pqGPC4Cx290uugLbIKo5EkTacVsrfYvq3X7SnBYuBiST+nNcRznqSv97ZJpRgFRm3v/YtjOa3gbboLgJ/Zftz2buA24C09blM17PRouyiyCqNxJInWmN8jtr/Q6/aUwfYnbS+wfTKtf6cf2W58D8n2L4FNkk5tv3U+L92pqakeA86RdGT7v8fzGYCbfAfT77MOejqPtr3zzRXAKn6zCmNtL9tUksXA+4GfSHqw/d6ftydCR/+5Eril/ct+I/DBHrfnkNm+V9Jy4AFas2DWMKDLcbezbdUPvXxewct7sr1nluBGRFSs10MHEREDL0EbEVGxBG1ERMUStBERFUvQRkRULEEbEVGxBG1ERMX+H4a8V9Niz4N9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = Agent()\n",
    "\n",
    "agent.SARSA(10000)\n",
    "\n",
    "plot_valuemap(agent.q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-waters",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-neutral",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
