{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "close-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ultimate-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps numeric actions to semantic actions\n",
    "action_map = {\n",
    "    1: \"UP\",\n",
    "    2: \"DOWN\",\n",
    "    3: \"LEFT\",\n",
    "    4: \"RIGHT\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "color-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input q-function --> Output value function\n",
    "# Used for plotting value grids with imshow\n",
    "def plot_valuemap(q):\n",
    "    v = {}\n",
    "    for row in range(7):\n",
    "        for col in range(10):\n",
    "            v[tuple((row,col))] = max([\n",
    "                q[tuple((row,col,1))],\n",
    "                q[tuple((row,col,2))], \n",
    "                q[tuple((row,col,3))], \n",
    "                q[tuple((row,col,4))], \n",
    "            ])\n",
    "    plt.imshow(np.array([*v.values()]).reshape(7,10))\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "clear-lebanon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridWorld Class\n",
    "class GridWorld:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        # Rewards\n",
    "        self.world = np.array([\n",
    "            [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "            [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "            [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "            [-1, -1, -1, -1, -1, -1, -1, +1, -1, -1],\n",
    "            [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "            [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "            [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "        ])\n",
    "        # Wind\n",
    "        self.wind = np.array([\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "        ])\n",
    "\n",
    "    # Input x, y, and numeric action --> Output new unclipped x, y\n",
    "    def step(self, x, y, action):\n",
    "        wind = self.wind[x][y]\n",
    "        if action == 1:\n",
    "            return x-1, y, wind\n",
    "        elif action == 2:\n",
    "            return x+1, y, wind\n",
    "        elif action == 3:\n",
    "            return x, y-1, wind\n",
    "        elif action == 4:\n",
    "            return x, y+1, wind\n",
    "        \n",
    "    # Input x, y --> Output reward\n",
    "    def reward(self, x, y, wind):\n",
    "        if x not in range(0,7) or y not in range(0,10):\n",
    "            return -1\n",
    "        else:\n",
    "            return self.world[np.clip(x-wind, 0, 6)][y]\n",
    "        \n",
    "    # Input x, y --> Output True if position is terminal state else False\n",
    "    def terminated(self, x, y):\n",
    "        return True if (x==3 and y==7) else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cross-characteristic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent class\n",
    "class Agent:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        self.gridworld = GridWorld()\n",
    "        self.x = 3\n",
    "        self.y = 0\n",
    "        self.x_prev = None\n",
    "        self.y_prev = None\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 0.1\n",
    "        self.S = []\n",
    "        self.q = {tuple([row, col, action]): np.random.normal(-1, 1) for row in range(self.gridworld.world.shape[0]) for col in range(self.gridworld.world.shape[1]) for action in action_map.keys()}\n",
    "        self.p = {tuple([row, col]): max(action_map.keys(), key = lambda a: self.q[tuple([row, col, a])]) for row in range(self.gridworld.world.shape[0]) for col in range(self.gridworld.world.shape[1])}\n",
    "        self.r = {tuple([row, col, action]): [] for row in range(self.gridworld.world.shape[0]) for col in range(self.gridworld.world.shape[1]) for action in action_map.keys()}\n",
    "    \n",
    "    # Wrapper function for GridWorld.step()\n",
    "    def step(self, action):\n",
    "        self.x_prev = self.x\n",
    "        self.y_prev = self.y\n",
    "        self.x, self.y, wind = self.gridworld.step(self.x, self.y, action)\n",
    "        reward = self.gridworld.reward(self.x, self.y, wind)\n",
    "        self.x = np.clip(self.x-wind, 0, 6)\n",
    "        self.y = np.clip(self.y, 0, 9)\n",
    "        return reward\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x = 3\n",
    "        self.y = 0\n",
    "    \n",
    "    # Wrapper function for GridWorld.reward()\n",
    "    def reward(self):\n",
    "        return self.gridworld.reward(self.x, self.y)\n",
    "    \n",
    "    # Wrapper function for GridWorld.terminated()\n",
    "    def terminated(self):\n",
    "        return self.gridworld.terminated(self.x, self.y)\n",
    "    \n",
    "    # Wrapper function for GridWorld.terminated()\n",
    "    def empty_s(self):\n",
    "        self.S = []\n",
    "    \n",
    "    # Monte Carlo control on-policy policy evaluation (for e-soft policies)\n",
    "    def MC(self, max_steps=np.inf, max_moves=100, epsilon=0.1, N=100):\n",
    "        axis_timesteps, axis_episodic = [], []\n",
    "        episode_count = 0\n",
    "        steps_count = 0\n",
    "        finished = False\n",
    "        while episode_count < N:\n",
    "            last_g = None\n",
    "            self.reset()\n",
    "            episode = []\n",
    "            for move in range(max_moves):\n",
    "                if np.random.choice([True, False], p=[epsilon, 1-epsilon]):\n",
    "                    action = random.choice(list(action_map.keys()))\n",
    "                else:\n",
    "                    action = self.p[tuple([self.x, self.y])]\n",
    "                reward = self.step(action)\n",
    "                steps_count+=1\n",
    "                episode.append(tuple([tuple([self.x_prev, self.y_prev]), action, reward]))\n",
    "                if self.terminated():\n",
    "                    axis_timesteps.append(steps_count)\n",
    "                    axis_episodic.append(episode_count)\n",
    "                    break\n",
    "                else:\n",
    "                    axis_timesteps.append(steps_count)\n",
    "                    axis_episodic.append(episode_count)\n",
    "                    \n",
    "            episode_count+=1\n",
    "            G = 0\n",
    "            for T, step in enumerate(reversed(episode)):\n",
    "                state, action, reward = step\n",
    "                G = self.gamma * G + reward\n",
    "                if not any([(state, action) == row for row in [(x[0], x[1]) for x in episode[:-T-1]]]):\n",
    "                    if state == tuple([3,0]):\n",
    "                        last_g = G\n",
    "#                     sa = tuple([state[0], state[1], action])\n",
    "#                     self.r[sa].append(G)\n",
    "#                     self.q[sa] = np.mean(self.r[sa])\n",
    "#                     self.p[state] = max(action_map.keys(), key = lambda a: self.q[tuple([state[0], state[1], a])])\n",
    "            self.S.append(last_g)\n",
    "                    \n",
    "    # SARSA on-policy (TD control)\n",
    "    def SARSA(self, N=10):\n",
    "        axis_timesteps, axis_episodic = [], []\n",
    "        episode_count = 0\n",
    "        step_count = 0\n",
    "        alpha = 0.5\n",
    "        epsilon = 0.1\n",
    "        self.q[tuple([3,7,1])] = 0\n",
    "        self.q[tuple([3,7,2])] = 0\n",
    "        self.q[tuple([3,7,3])] = 0\n",
    "        self.q[tuple([3,7,4])] = 0\n",
    "        while episode_count < N:\n",
    "            self.reset()\n",
    "            while True:\n",
    "                if np.random.choice([True, False], p=[epsilon, 1-epsilon]):\n",
    "                    A1 = random.choice(list(action_map.keys()))\n",
    "                else:\n",
    "                    A1 = max(action_map.keys(), key = lambda a: self.q[tuple([self.x, self.y, a])])\n",
    "                reward = self.step(A1)\n",
    "                if np.random.choice([True, False], p=[epsilon, 1-epsilon]):\n",
    "                    A2 = random.choice(list(action_map.keys()))\n",
    "                else:\n",
    "                    A2 = max(action_map.keys(), key = lambda a: self.q[tuple([self.x, self.y, a])])\n",
    "                step_count+=1\n",
    "                sa1 = tuple([self.x_prev, self.y_prev, A1])\n",
    "                sa2 = tuple([self.x, self.y, A2])\n",
    "                self.q[sa1] = self.q[sa1] + alpha * ( reward + self.gamma * self.q[sa2] - self.q[sa1])\n",
    "                if self.terminated():\n",
    "                    episode_count+=1\n",
    "                    axis_timesteps.append(step_count)\n",
    "                    axis_episodic.append(episode_count)\n",
    "                    break\n",
    "                else:\n",
    "                    axis_timesteps.append(step_count)\n",
    "                    axis_episodic.append(episode_count)\n",
    "        for row in range(7):\n",
    "            for col in range(10):\n",
    "                agent.p[tuple([row,col])] = max(action_map.keys(), key = lambda a: agent.q[tuple([row, col, a])])\n",
    "    \n",
    "    # n-step SARSA with n = 4\n",
    "    def nstepSARSA(self, n=4, N=100):\n",
    "        axis_timesteps, axis_episodic = [], []\n",
    "        episode_count = 0\n",
    "        step_count = 0\n",
    "        lr = 0.5\n",
    "        epsilon = 0.1\n",
    "        finished = False\n",
    "        while episode_count < N:\n",
    "            self.reset()\n",
    "            t = 0\n",
    "            T = np.inf\n",
    "            action = self.p[tuple([self.x, self.y])]\n",
    "            actions = [action]\n",
    "            states = [tuple([self.x, self.y])]\n",
    "            rewards = [0]\n",
    "            while True:\n",
    "                if t < T:\n",
    "                    reward = self.step(action)\n",
    "                    state = tuple([self.x, self.y])\n",
    "\n",
    "                    states.append(state)\n",
    "                    rewards.append(reward)\n",
    "\n",
    "                    if self.terminated():\n",
    "                        episode_count+=1\n",
    "                        T = t + 1\n",
    "                    else:\n",
    "                        if np.random.choice([True, False], p=[epsilon, 1-epsilon]):\n",
    "                            action = random.choice(list(action_map.keys()))\n",
    "                        else:\n",
    "                            action = max(action_map.keys(), key = lambda a: self.q[tuple([self.x, self.y, a])])\n",
    "                        actions.append(action)  \n",
    "                tau = t - n + 1\n",
    "                step_count+=1\n",
    "                axis_timesteps.append(step_count)\n",
    "                axis_episodic.append(episode_count)\n",
    "                if tau >= 0:\n",
    "                    G = 0\n",
    "                    for i in range(tau + 1, min(tau + n + 1, T + 1)):\n",
    "                        G += np.power(self.gamma, i - tau - 1) * rewards[i]\n",
    "                    if tau + n < T:\n",
    "                        state_action = (states[tau + n], actions[tau + n])\n",
    "                        state_action = [*state_action[0]] + [state_action[1]]\n",
    "                        G += np.power(self.gamma, n) * self.q[tuple(state_action)]\n",
    "                    if tau == 0:\n",
    "                        self.S.append(G)\n",
    "#                     state_action = (states[tau], actions[tau])\n",
    "#                     state_action = [*state_action[0]] + [state_action[1]]\n",
    "#                     self.q[tuple(state_action)] += lr * (\n",
    "#                                 G - self.q[tuple(state_action)])\n",
    "                if tau == T - 1:\n",
    "                    break\n",
    "\n",
    "                t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-accounting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAADpCAYAAABhjS8kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAASEUlEQVR4nO3df6xfdX3H8derty2lF4FiQZSWwaKyOKbi7pjI4qYgY/5ic5rpoovOpDEZDhcTIzaZmcuSJW5OE43xDtFlMnWpEI1jFIgYYoJoqQyB4iRMpfVHKYpgCfT2fl/74/utudIf93x7z+fzPef2+UhO6Pfe8z2f97e0r/vp53w+5+MkAgC0b8WkCwCA5YqABYBCCFgAKISABYBCCFgAKGTlpAsAgKX4w5dN5+Gfzjc69467ntya5NLCJf0SAQug1x7+6by+sfXMRudOPfO76wuX8ysIWAC9FkkDDSZdxiERsAB6LYrm0myIoDYCFkDvdbUHyywCAL0WRfNpdizG9tW2d9u+u43aCFgAvTdQGh0NfFpSa7MMGCIA0GuRNN8sPBe/VnKr7bNauZgIWAA9F0lz6eYYLAELoPfGiNf1trcteD2bZLb1gkYIWAC9FmWcIYI9SWZK1rMQAQug3yLNd3TfAGYRAOi14UquZsdibH9W0m2SzrG90/bbl1IbPVgAPWfNy61cKcmbWrnQCAELoNeGswjaCdi2FQnYlWumc9z0KSUufRBXnJ2xYq5OY963v0o7kpS5uWpteareiFQG9f5g2JU+16p6/aGsnKrSzhNPPqJ9c3uXlI7DebDHUMAeN32KfvNV7ypx6YOs2ltvdPv4nzxRpZ1VP9hTpR1J2r9zV7W2pk44sVpbg8cfr9aWV6+u086znlGlHUnaf1qd/1ffuPPjrVxncCz1YAGglmOuBwsAtUTWfEcnRBGwAHqPIQIAKCCy9qXOTblxEbAAem240IAhAgAogptcAFBAYs2HHiwAFDGgBwsA7Rve5OpmlDXqV9u+1PZ3bN9v+72liwKApg7c5Gpy1LZo7NuekvQxSa+QtFPSN21/Kcm9pYsDgCbmezwP9nxJ9yd5QJJsf07SZZIIWAAT1/eVXGdIenDB652SfvepJ9neJGmTJK2eXtdKcQDQxGC5zyIYbRw2K0nTT9/Y0Q0cACw3w4e99Ddgd0nauOD1htHXAGDiImuux0tlvynpObbP1jBY3yjpz4tWBQANJervQoMk+21fLmmrpClJVye5p3hlANCI+73QIMn1kq4vXAsAjC3qcQ8WALquzze5AKCzIvPAbQAoYbhtdzejrJtVAUBj5nmwAFBCdAys5AKASaEHCwAFJD7GerCWBlN1fqI8eWK9n1xZsaZKO4MzNy5+UkvmV59Zra1HnlutKc2d+WS9xh5ZVaWZ9dvrhcian81XaWewaul/f4c3ubq5VLabsQ8AjQ335GpyNLpaixsMMEQAoNeGN7na+Zds2xsMELAAeq/FlVytbjBAwALotTFXcq23vW3B69nRs6wPaLTBQFMELIDeG2NDwz1JZkrWshABC6DXEmlu0NoQQasbDBCwAHptOETQWsC2usEAAQug99paydX2BgMELIBea3OaltTuBgOL9qttX217t+2722gQANo1HCJoctTWpMVPS7q0cB0AcFQSaS4rGh21Ndn08FbbZ5UvBQCOzrJ/2IvtTZI2SdLq6XVtXRYAjqjLW8a0FvtJZpPMJJlZuWa6rcsCwKIGo627FztqYxYBgF5rexZBmwhYAL3X1THYJtO0PivpNknn2N5p++3lywKAZhJrf1Y0OmprMovgTTUKAYCjxRABABTAGCwAFETAAkABXZ4HS8AC6L1JzHFtgoAF0GuJtL+9B263ioAF0HsMEQBAAYzBAkBBOZYCNpYGq0tc+WBza+v9xs5NT1Vp5/iHB1XakaTdF+6v1tZrfvvOam29YPrBxU9qyYf+/XVV2nn011OlHUma+3GdvtdgVTt/f7nJBQAFJNI8N7kAoATGYAGgmGNqDBYAauFZBABQSobjsF1EwALoPWYRAEABkZlFAAClMEQAAIV0dRZBkz25Ntq+xfa9tu+xfUWNwgCgiWQYsE2O2pr0YPdLeneS7bafJukO2zclubdwbQDQSFenaS3ag03yoyTbR79+TNIOSWeULgwAmkqaHUth+w2jf8UPbM80ec9YY7C2z5J0nqTbD/G9TZI2SdKqE9aNc1kAOGqRNagzi+BuSa+T9Immb2gcsLZPkPQFSe9K8uhTv59kVtKsJK09dWNH7+kBWI5qBE6SHZJkNx+OaBSwtldpGK7XJLn2qKoDgBIy1iyC9ba3LXg9O+ocFrFowHoY15+UtCPJh0oVAgBHrXkXdk+Sw46f2r5Z0umH+NbmJF8ct6wmPdgLJb1F0rdtH3hi8vuSXD9uYwBQQltTsJJc3MqFRhYN2CRfkzq60BfAMS+SBoNuRlQ3F/ACQFPRcJ+qJscS2P4T2zslXSDpv2xvXew9LJUF0Hs1nkWQ5DpJ143zHgIWQP91dGIoAQug5ybznIEmCFgA/UcPFgAKiJSOziIgYAEsAwQsAJRxLA0RZKX0xCl1fqIMVldpZtRYnWYeObdSQ5Ke9oxfVGvr6av2VmvrJ3MnVWvrla+/rUo7W77xO1XakaSsmKrSzmBVSxc6lgIWAKo5sNCggwhYAL3HpocAUAqzCACgDNODBYACIm5yAUAZS39SVikELID+owcLAIXUmzo+FgIWQL/1eR6s7TWSbpV03Oj8LUneX7owAGiqz7MInpT08iS/GG3f/TXb/53k64VrA4Bm+hqwSSLpwIL1VaOjox8HALqj0aaHtqdGW3bvlnRTktsPcc4m29tsb9v/eL2HegCA0+yorVHAJplP8kJJGySdb/vcQ5wzm2QmyczKtdNt1wkAhxYNl8o2OSoba9vuJI9IukXSpWXKAYCjkIZHZYsGrO1TbZ88+vXxkl4h6b7ShQFAU10dImgyi+CZkv7N9pSGgfyfSb5ctiwAGENHb7s3mUVwl6TzKtQCAEenrwELAF3mSOZ5sABQCD1YACijz0tlAaDbOhqwY82DBYDOaThFa6m9XNsftH2f7btsX3dg+uqRELAA+q/OQoObJJ2b5PmS/lfSlYu9gYAF0HseNDuWIsmNSfaPXn5dw0cHHFGRMdjBSumJ9XUGRZb6mzaOwapKAz1T9QaU5ufr/Yz9yb4Tq7V1ycl3V2vrtl88u0o7f/cH11ZpR5I+cMdrqrST46pvRbDe9rYFr2eTzB7Fdf5S0ucXO4mbXAD6r3mfZE+SmcN90/bNkk4/xLc2J/ni6JzNkvZLumaxxghYAP3W4nMGklx8pO/bfqukV0u6aPSs7CMiYAH0X4VRNduXSnqPpN9P8niT9xCwAPqvzm2Lj2q4N+FNtiXp60necaQ3ELAAes2qc7M7ydh3MwlYAP02oWe9NkHAAug/AhYACiFgAaAMhggAoIRIqr4grJnG6yRtT9n+lm324wLQKV3d9HCchehXSNpRqhAAOGp93bZbkmxvkPQqSVeVLQcAxtf3HuyHNVwidtiRDtubbG+zvW1+795WigOARvrag7X9akm7k9xxpPOSzCaZSTIzNT3dWoEAcERNw3UCAdtkFsGFkl5r+5WS1kg60fZnkry5bGkAsDiru9O0Fu3BJrkyyYYkZ0l6o6SvEK4AuqSrY7DMgwXQfx3twY4VsEm+KumrRSoBgKO1HAIWADqHp2kBQEEELACUUXN36XEQsAB6jyECAChhQosImiBgAfQfAQsA7evySq4yARvJ80WufJCVT7hOQ5L2ra70f3FQ7zPNzU1Va+ulJ36nWluXHP/Tam19YMc5VdrZPr2xSjuSdMULv1KlnX9e+1gr1/GgmwlLDxZAvzEGCwDlHFtDBABQEwELAGXQgwWAUghYACggLJUFgCK6PA92nG27AaCbkmbHEtj+e9t32b7T9o22n7XYewhYAL1XacuYDyZ5fpIXSvqypL9d7A2Nhghsf0/SY5LmJe1PMrOUKgGgNZUWGiR5dMHL6SatjjMG+7Ike8auCgAKG+Mm13rb2xa8nk0y27gd+x8k/YWkn0t62WLnc5MLQO+NEbB7jvQvcNs3Szr9EN/anOSLSTZL2mz7SkmXS3r/kRprGrCRdKPtSPrEOIkPAEVFS76B9ctLJRc3PPUaSderpYD9vSS7bJ8m6Sbb9yW5deEJtjdJ2iRJK09e1/CyALB0NaZp2X5Oku+OXl4m6b7F3tMoYJPsGv13t+3rJJ0v6dannDMraVaSjtuwsaOz0gAsS3US5x9tnyNpIOn7kt6x2BsWDVjb05JWJHls9OtLJH1gqZUCQBtqLTRI8qfjvqdJD/YZkq6zfeD8/0hyw7gNAUARSX8fuJ3kAUkvqFALABydbuYr07QA9F9Xn0VAwALot0jq6xABAHReN/OVgAXQfwwRAEAhvZ1FAACdxrbdAFDGcKFBNxOWgAXQf8fSnlyONPWkS1z6IHPT9X5yZd1clXZOXre3SjuSdPlzv1qtrTec8HC1tv7152dXa2v7zOertPPsW95WpR1J2nlKnQc27Us7EUQPFgBKYAwWAErp8bMIAKDzGCIAgAIy1pYxVRGwAPqPHiwAFNLNfCVgAfSfB90cIyBgAfRbdGwtNACAWqx0dqHBiiYn2T7Z9hbb99neYfuC0oUBQGNJs6Oypj3Yj0i6Icnrba+WtLZgTQAwno72YJts232SpJdKeqskJdknaV/ZsgCgoQ6PwTYZIjhb0kOSPmX7W7avsj391JNsb7K9zfa2+b31HlYCAB4MGh21NQnYlZJeJOnjSc6TtFfSe596UpLZJDNJZqamD8pfACik4fjrBIYRmgTsTkk7k9w+er1Fw8AFgMmL+huwSX4s6UHb54y+dJGke4tWBQDjGDQ8Kms6i+Cdkq4ZzSB4QFK9J/8CwCK6Og+2UcAmuVPSTOFaAGB8kTTfzWkEjRYaAEB31b3JZfvdtmN7/WLnslQWQP9VGiKwvVHSJZJ+0OR8erAA+q9eD/ZfJL1HDR+QSA8WQL9FUvM9udbb3rbg9WyS2SZvtH2ZpF1J/sdutms2AQug5yKl8U2uPUkOe8Pe9s2STj/EtzZLep+GwwONEbAA+q3FWQRJLj7U123/loaPDTjQe90gabvt80drBQ6JgAXQf4VvciX5tqTTDry2/T1JM0n2HOl9BCyA/uvoQgOnQGG2H5L0/THftl7SEX8a9NRy/FzL8TNJy/Nzdf0z/VqSU5dygZNWn5aXnPpnjc694YcfveNIY7BtK9KDPZrfMNvban7wWpbj51qOn0lanp9rOX6mg0QSmx4CQCEdHSIgYAH0XDr7LIIuBWyjyb49tBw/13L8TNLy/FzL8TP9qkhpPg+2qiI3uQCglpNWnpoLTvzjRudu/dlV/b/JBQBVdbSjSMAC6Leks7MIJv40LduX2v6O7fttH7SZYh/Z3mj7Ftv32r7H9hWTrqkttqdGuwt/edK1tMX2yba32L7P9g7bF0y6pjbY/pvRn7+7bX/W9ppJ11RK5ucbHbVNNGBtT0n6mKQ/kvQ8SW+y/bxJ1tSS/ZLeneR5kl4s6a+WyeeSpCsk7Zh0ES37iKQbkvyGpBdoGXw+22dI+msNl3OeK2lK0hsnW1Up/d5VtqTzJd2f5IEk+yR9TtJlE65pyZL8KMn20a8f0/Av7BmTrWrpbG+Q9CpJV026lrbYPknSSyV9UpKS7EvyyGSras1KScfbXilpraQfTrieMg48rrDJUdmkA/YMSQ8ueL1TyyCIFrJ9lqTzJN1+5DN74cMaPmy4mwNeR+dsSQ9J+tRo6OMq29OTLmqpkuyS9E8aPnn/R5J+nuTGyVZVUAbNjsomHbDLmu0TJH1B0ruSPDrpepbC9qsl7U5yx6RradlKSS+S9PEk50naK6n39wJsr9PwX4NnS3qWpGnbb55sVWVEUgZpdNQ26YDdJWnjgtcbRl/rPdurNAzXa5JcO+l6WnChpNeOHtP2OUkvt/2ZyZbUip2SdiY58C+MLRoGbt9dLOn/kjyUZE7StZJeMuGaykjowR7GNyU9x/bZtldrOAj/pQnXtGQePpH3k5J2JPnQpOtpQ5Irk2xIcpaG/5++kqT3PaLRw5IftH3O6EsXSbp3giW15QeSXmx77ejP40VaBjfvDqerswgmOg82yX7bl0vaquFdzquT3DPJmlpyoaS3SPq27TtHX3tfkusnWBMO752Srhn9kH9A0tsmXM+SJbnd9hZJ2zWc1fItLdNls4/pZ1tvzpZFt9AeqfroRpbKAkAhkx4iAIBli4AFgEIIWAAohIAFgEIIWAAohIAFgEIIWAAo5P8BdzH2xIotTecAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = Agent()\n",
    "agent.SARSA(N=1)\n",
    "plot_valuemap(agent.q)\n",
    "\n",
    "minimum = +np.Inf\n",
    "maximum = -np.Inf\n",
    "\n",
    "agent.MC()\n",
    "fig1 = px.histogram(agent.S)\n",
    "minimum = min(min(agent.S), minimum)\n",
    "maximum = max(max(agent.S), maximum)\n",
    "agent.empty_s()\n",
    "\n",
    "agent.nstepSARSA(n=1)\n",
    "fig2 = px.histogram(agent.S)\n",
    "minimum = min(min(agent.S), minimum)\n",
    "maximum = max(max(agent.S), maximum)\n",
    "agent.empty_s()\n",
    "\n",
    "agent.nstepSARSA(n=4)\n",
    "fig3 = px.histogram(agent.S)\n",
    "minimum = min(min(agent.S), minimum)\n",
    "maximum = max(max(agent.S), maximum)\n",
    "agent.empty_s()\n",
    "\n",
    "fig1.update_layout(\n",
    "    title_text='MONTE CARLO N=1: G-Distribution for S = (3,0)', # title of plot\n",
    "    xaxis_title_text='G', # xaxis label\n",
    "    yaxis_title_text='Count', # yaxis label\n",
    "    bargap=0.2, # gap between bars of adjacent location coordinates\n",
    "    bargroupgap=0.1, # gap between bars of the same location coordinates\n",
    "    showlegend=False,\n",
    "    xaxis_range=[minimum,maxmimum]\n",
    "\n",
    ")\n",
    "\n",
    "fig2.update_layout(\n",
    "    title_text='TD(0) N=1: R-Distribution for S = (3,0)', # title of plot\n",
    "    xaxis_title_text='G', # xaxis label\n",
    "    yaxis_title_text='Count', # yaxis label\n",
    "    bargap=0.2, # gap between bars of adjacent location coordinates\n",
    "    bargroupgap=0.1, # gap between bars of the same location coordinates\n",
    "    showlegend=False,\n",
    "    xaxis_range=[minimum,maxmimum]\n",
    ")\n",
    "\n",
    "fig3.update_layout(\n",
    "    title_text='NSTEP 4 N=1: R-Distribution for S = (3,0)', # title of plot\n",
    "    xaxis_title_text='G', # xaxis label\n",
    "    yaxis_title_text='Count', # yaxis label\n",
    "    bargap=0.2, # gap between bars of adjacent location coordinates\n",
    "    bargroupgap=0.1, # gap between bars of the same location coordinates\n",
    "    showlegend=False,\n",
    "    xaxis_range=[minimum,maxmimum]\n",
    ")\n",
    "\n",
    "fig1.show()\n",
    "fig2.show()\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()\n",
    "agent.SARSA(N=10)\n",
    "plot_valuemap(agent.q)\n",
    "\n",
    "minimum = +np.Inf\n",
    "maximum = -np.Inf\n",
    "\n",
    "agent.MC()\n",
    "fig1 = px.histogram(agent.S)\n",
    "minimum = min(min(agent.S), minimum)\n",
    "maximum = max(max(agent.S), maximum)\n",
    "agent.empty_s()\n",
    "\n",
    "agent.nstepSARSA(n=1)\n",
    "fig2 = px.histogram(agent.S)\n",
    "minimum = min(min(agent.S), minimum)\n",
    "maximum = max(max(agent.S), maximum)\n",
    "agent.empty_s()\n",
    "\n",
    "agent.nstepSARSA(n=4)\n",
    "fig3 = px.histogram(agent.S)\n",
    "minimum = min(min(agent.S), minimum)\n",
    "maximum = max(max(agent.S), maximum)\n",
    "agent.empty_s()\n",
    "\n",
    "fig1.update_layout(\n",
    "    title_text='MONTE CARLO N=10: G-Distribution for S = (3,0)', # title of plot\n",
    "    xaxis_title_text='G', # xaxis label\n",
    "    yaxis_title_text='Count', # yaxis label\n",
    "    bargap=0.2, # gap between bars of adjacent location coordinates\n",
    "    bargroupgap=0.1, # gap between bars of the same location coordinates\n",
    "    showlegend=False,\n",
    "    xaxis_range=[minimum,maxmimum]\n",
    "\n",
    ")\n",
    "\n",
    "fig2.update_layout(\n",
    "    title_text='TD(0) N=10: R-Distribution for S = (3,0)', # title of plot\n",
    "    xaxis_title_text='G', # xaxis label\n",
    "    yaxis_title_text='Count', # yaxis label\n",
    "    bargap=0.2, # gap between bars of adjacent location coordinates\n",
    "    bargroupgap=0.1, # gap between bars of the same location coordinates\n",
    "    showlegend=False,\n",
    "    xaxis_range=[minimum,maxmimum]\n",
    ")\n",
    "\n",
    "fig3.update_layout(\n",
    "    title_text='NSTEP 4 N=10: R-Distribution for S = (3,0)', # title of plot\n",
    "    xaxis_title_text='G', # xaxis label\n",
    "    yaxis_title_text='Count', # yaxis label\n",
    "    bargap=0.2, # gap between bars of adjacent location coordinates\n",
    "    bargroupgap=0.1, # gap between bars of the same location coordinates\n",
    "    showlegend=False,\n",
    "    xaxis_range=[minimum,maxmimum]\n",
    ")\n",
    "\n",
    "fig1.show()\n",
    "fig2.show()\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()\n",
    "agent.SARSA(N=50)\n",
    "plot_valuemap(agent.q)\n",
    "\n",
    "minimum = +np.Inf\n",
    "maximum = -np.Inf\n",
    "\n",
    "agent.MC()\n",
    "fig1 = px.histogram(agent.S)\n",
    "minimum = min(min(agent.S), minimum)\n",
    "maximum = max(max(agent.S), maximum)\n",
    "agent.empty_s()\n",
    "\n",
    "agent.nstepSARSA(n=1)\n",
    "fig2 = px.histogram(agent.S)\n",
    "minimum = min(min(agent.S), minimum)\n",
    "maximum = max(max(agent.S), maximum)\n",
    "agent.empty_s()\n",
    "\n",
    "agent.nstepSARSA(n=4)\n",
    "fig3 = px.histogram(agent.S)\n",
    "minimum = min(min(agent.S), minimum)\n",
    "maximum = max(max(agent.S), maximum)\n",
    "agent.empty_s()\n",
    "\n",
    "fig1.update_layout(\n",
    "    title_text='MONTE CARLO N=50: G-Distribution for S = (3,0)', # title of plot\n",
    "    xaxis_title_text='G', # xaxis label\n",
    "    yaxis_title_text='Count', # yaxis label\n",
    "    bargap=0.2, # gap between bars of adjacent location coordinates\n",
    "    bargroupgap=0.1, # gap between bars of the same location coordinates\n",
    "    showlegend=False,\n",
    "    xaxis_range=[minimum,maxmimum]\n",
    "\n",
    ")\n",
    "\n",
    "fig2.update_layout(\n",
    "    title_text='TD(0) N=50: R-Distribution for S = (3,0)', # title of plot\n",
    "    xaxis_title_text='G', # xaxis label\n",
    "    yaxis_title_text='Count', # yaxis label\n",
    "    bargap=0.2, # gap between bars of adjacent location coordinates\n",
    "    bargroupgap=0.1, # gap between bars of the same location coordinates\n",
    "    showlegend=False,\n",
    "    xaxis_range=[minimum,maxmimum]\n",
    ")\n",
    "\n",
    "fig3.update_layout(\n",
    "    title_text='NSTEP 4 N=50: R-Distribution for S = (3,0)', # title of plot\n",
    "    xaxis_title_text='G', # xaxis label\n",
    "    yaxis_title_text='Count', # yaxis label\n",
    "    bargap=0.2, # gap between bars of adjacent location coordinates\n",
    "    bargroupgap=0.1, # gap between bars of the same location coordinates\n",
    "    showlegend=False,\n",
    "    xaxis_range=[minimum,maxmimum]\n",
    ")\n",
    "\n",
    "fig1.show()\n",
    "fig2.show()\n",
    "fig3.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
