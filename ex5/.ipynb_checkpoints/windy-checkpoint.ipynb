{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "documentary-cross",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "corresponding-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps numeric actions to semantic actions\n",
    "action_map = {\n",
    "    1: \"UP\",\n",
    "    2: \"DOWN\",\n",
    "    3: \"LEFT\",\n",
    "    4: \"RIGHT\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fifty-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input q-function --> Output value function\n",
    "# Used for plotting value grids with imshow\n",
    "def plot_valuemap(q):\n",
    "    v = {}\n",
    "    for row in range(7):\n",
    "        for col in range(10):\n",
    "            v[tuple((row,col))] = max([\n",
    "                q[tuple((row,col,1))],\n",
    "                q[tuple((row,col,2))], \n",
    "                q[tuple((row,col,3))], \n",
    "                q[tuple((row,col,4))], \n",
    "            ])\n",
    "    plt.imshow(np.array([*v.values()]).reshape(7,10))\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "binary-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridWorld Class\n",
    "class GridWorld:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        # Rewards\n",
    "        self.world = np.array([\n",
    "            [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "            [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "            [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "            [-1, -1, -1, -1, -1, -1, -1, +1, -1, -1],\n",
    "            [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "            [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "            [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "        ])\n",
    "        # Wind\n",
    "        self.wind = np.array([\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "            [0, 0, 0, 1, 1, 1, 2, 2, 1, 0],\n",
    "        ])\n",
    "\n",
    "    # Input x, y, and numeric action --> Output new unclipped x, y\n",
    "    def step(self, x, y, action):\n",
    "        wind = self.wind[x][y]\n",
    "        if action == 1:\n",
    "            return x-1, y, wind\n",
    "        elif action == 2:\n",
    "            return x+1, y, wind\n",
    "        elif action == 3:\n",
    "            return x, y-1, wind\n",
    "        elif action == 4:\n",
    "            return x, y+1, wind\n",
    "        \n",
    "    # Input x, y --> Output reward\n",
    "    def reward(self, x, y, wind):\n",
    "        if x not in range(0,7) or y not in range(0,10):\n",
    "            return -1\n",
    "        else:\n",
    "            return self.world[np.clip(x-wind, 0, 6)][y]\n",
    "        \n",
    "    # Input x, y --> Output True if position is terminal state else False\n",
    "    def terminated(self, x, y):\n",
    "        return True if (x==3 and y==7) else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "trained-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent class\n",
    "class Agent:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        self.gridworld = GridWorld()\n",
    "        self.x = 3\n",
    "        self.y = 0\n",
    "        self.x_prev = None\n",
    "        self.y_prev = None\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 0.1\n",
    "        self.q = {tuple([row, col, action]): np.random.normal(-1, 1) for row in range(self.gridworld.world.shape[0]) for col in range(self.gridworld.world.shape[1]) for action in action_map.keys()}\n",
    "        self.p = {tuple([row, col]): max(action_map.keys(), key = lambda a: self.q[tuple([row, col, a])]) for row in range(self.gridworld.world.shape[0]) for col in range(self.gridworld.world.shape[1])}\n",
    "        self.r = {tuple([row, col, action]): [] for row in range(self.gridworld.world.shape[0]) for col in range(self.gridworld.world.shape[1]) for action in action_map.keys()}\n",
    "    \n",
    "    # Wrapper function for GridWorld.step()\n",
    "    def step(self, action):\n",
    "        self.x_prev = self.x\n",
    "        self.y_prev = self.y\n",
    "        self.x, self.y, wind = self.gridworld.step(self.x, self.y, action)\n",
    "        reward = self.gridworld.reward(self.x, self.y, wind)\n",
    "        self.x = np.clip(self.x-wind, 0, 6)\n",
    "        self.y = np.clip(self.y, 0, 9)\n",
    "        return reward\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x = 3\n",
    "        self.y = 0\n",
    "    \n",
    "    # Wrapper function for GridWorld.episode()\n",
    "    def episode(self):\n",
    "        return self.gridworld.episode(self.p)\n",
    "    \n",
    "    # Wrapper function for GridWorld.reward()\n",
    "    def reward(self):\n",
    "        return self.gridworld.reward(self.x, self.y)\n",
    "    \n",
    "    # Wrapper function for GridWorld.terminated()\n",
    "    def terminated(self):\n",
    "        return self.gridworld.terminated(self.x, self.y)\n",
    "    \n",
    "    # Monte Carlo control on-policy policy evaluation (for e-soft policies)\n",
    "    def MC(self, max_steps=8000, max_moves=1000, epsilon=0.1):\n",
    "        axis_timesteps, axis_episodic = [], []\n",
    "        episode_count = 0\n",
    "        steps_count = 0\n",
    "        finished = False\n",
    "        while not finished:\n",
    "            self.reset()\n",
    "            episode = []\n",
    "            for move in range(max_moves):\n",
    "                if not steps_count<max_steps:\n",
    "                    finished=True\n",
    "                    break\n",
    "                if np.random.choice([True, False], p=[epsilon, 1-epsilon]):\n",
    "                    action = random.choice(list(action_map.keys()))\n",
    "                else:\n",
    "                    action = self.p[tuple([self.x, self.y])]\n",
    "                reward = self.step(action)\n",
    "                steps_count+=1\n",
    "                episode.append(tuple([tuple([self.x_prev, self.y_prev]), action, reward]))\n",
    "                if self.terminated():\n",
    "                    episode_count+=1\n",
    "                    axis_timesteps.append(steps_count)\n",
    "                    axis_episodic.append(episode_count)\n",
    "                    break\n",
    "                else:\n",
    "                    axis_timesteps.append(steps_count)\n",
    "                    axis_episodic.append(episode_count)\n",
    "            G = 0\n",
    "            for T, step in enumerate(reversed(episode)):\n",
    "                state, action, reward = step\n",
    "                G = self.gamma * G + reward\n",
    "                if not any([(state, action) == row for row in [(x[0], x[1]) for x in episode[:-T-1]]]):\n",
    "                    sa = tuple([state[0], state[1], action])\n",
    "                    self.r[sa].append(G)\n",
    "                    self.q[sa] = np.mean(self.r[sa])\n",
    "                    self.p[state] = max(action_map.keys(), key = lambda a: self.q[tuple([state[0], state[1], a])])\n",
    "        return axis_timesteps, axis_episodic\n",
    "                    \n",
    "    # SARSA on-policy (TD control)\n",
    "    def SARSA(self, max_steps=8000):\n",
    "        axis_timesteps, axis_episodic = [], []\n",
    "        episode_count = 0\n",
    "        step_count = 0\n",
    "        alpha = 0.5\n",
    "        epsilon = 0.1\n",
    "        self.q[tuple([3,7,1])] = 0\n",
    "        self.q[tuple([3,7,2])] = 0\n",
    "        self.q[tuple([3,7,3])] = 0\n",
    "        self.q[tuple([3,7,4])] = 0\n",
    "        finished = False\n",
    "        while not finished:\n",
    "            self.reset()\n",
    "            for step in range(max_steps):\n",
    "                if not step_count < max_steps:\n",
    "                    finished = True\n",
    "                    break\n",
    "                if np.random.choice([True, False], p=[epsilon, 1-epsilon]):\n",
    "                    A1 = random.choice(list(action_map.keys()))\n",
    "                else:\n",
    "                    A1 = max(action_map.keys(), key = lambda a: self.q[tuple([self.x, self.y, a])])\n",
    "                reward = self.step(A1)\n",
    "                if np.random.choice([True, False], p=[epsilon, 1-epsilon]):\n",
    "                    A2 = random.choice(list(action_map.keys()))\n",
    "                else:\n",
    "                    A2 = max(action_map.keys(), key = lambda a: self.q[tuple([self.x, self.y, a])])\n",
    "                step_count+=1\n",
    "                sa1 = tuple([self.x_prev, self.y_prev, A1])\n",
    "                sa2 = tuple([self.x, self.y, A2])\n",
    "                self.q[sa1] = self.q[sa1] + alpha * ( reward + self.gamma * self.q[sa2] - self.q[sa1])\n",
    "                if self.terminated():\n",
    "                    episode_count+=1\n",
    "                    axis_timesteps.append(step_count)\n",
    "                    axis_episodic.append(episode_count)\n",
    "                    break\n",
    "                else:\n",
    "                    axis_timesteps.append(step_count)\n",
    "                    axis_episodic.append(episode_count)\n",
    "                    \n",
    "        return axis_timesteps, axis_episodic\n",
    "               \n",
    "    # Q-Learning off-policy (TD control)\n",
    "    def qLearning(self, max_steps=8000):\n",
    "        axis_timesteps, axis_episodic = [], []\n",
    "        episode_count = 0\n",
    "        step_count = 0\n",
    "        alpha = 0.5\n",
    "        epsilon = 0.1\n",
    "        self.q[tuple([3,7,1])] = 0\n",
    "        self.q[tuple([3,7,2])] = 0\n",
    "        self.q[tuple([3,7,3])] = 0\n",
    "        self.q[tuple([3,7,4])] = 0\n",
    "        finished = False\n",
    "        while not finished:\n",
    "            self.reset()\n",
    "            for step in range(max_steps):\n",
    "                if not step_count < max_steps:\n",
    "                    finished = True\n",
    "                    break\n",
    "                if np.random.choice([True, False], p=[epsilon, 1-epsilon]):\n",
    "                    A = random.choice(list(action_map.keys()))\n",
    "                else:\n",
    "                    A = max(action_map.keys(), key = lambda a: self.q[tuple([self.x, self.y, a])])\n",
    "                reward = self.step(A)\n",
    "                step_count+=1\n",
    "                sa = tuple([self.x_prev, self.y_prev, A])\n",
    "                next_action = max(action_map.keys(), key = lambda a: self.q[tuple([self.x, self.y, a])])\n",
    "                self.q[sa] = self.q[sa] + alpha * ( reward + self.gamma * self.q[tuple([self.x, self.y, next_action])] - self.q[sa])\n",
    "                if self.terminated():\n",
    "                    episode_count+=1\n",
    "                    axis_timesteps.append(step_count)\n",
    "                    axis_episodic.append(episode_count)\n",
    "                    break\n",
    "                else:\n",
    "                    axis_timesteps.append(step_count)\n",
    "                    axis_episodic.append(episode_count)\n",
    "                    \n",
    "        return axis_timesteps, axis_episodic\n",
    "    \n",
    "    # Expected SARSA\n",
    "    def expSARSA(self, max_steps=8000):\n",
    "        axis_timesteps, axis_episodic = [], []\n",
    "        episode_count = 0\n",
    "        step_count = 0\n",
    "        alpha = 0.5\n",
    "        epsilon = 0.1\n",
    "        n_actions = len(action_map)\n",
    "        self.q[tuple([3,7,1])] = 0\n",
    "        self.q[tuple([3,7,2])] = 0\n",
    "        self.q[tuple([3,7,3])] = 0\n",
    "        self.q[tuple([3,7,4])] = 0\n",
    "        finished = False\n",
    "        while not finished:\n",
    "            self.reset()\n",
    "            for step in range(max_steps):\n",
    "                if not step_count < max_steps:\n",
    "                    finished = True\n",
    "                    break\n",
    "                if np.random.choice([True, False], p=[epsilon, 1-epsilon]):\n",
    "                    A = random.choice(list(action_map.keys()))\n",
    "                else:\n",
    "                    A = max(action_map.keys(), key = lambda a: self.q[tuple([self.x, self.y, a])])\n",
    "                reward = self.step(A)\n",
    "                step_count+=1\n",
    "                sa = tuple([self.x_prev, self.y_prev, A])\n",
    "                actions = [*action_map.keys()]\n",
    "                next_action = max(action_map.keys(), key = lambda a: self.q[tuple([self.x, self.y, a])])\n",
    "                actions.remove(next_action)\n",
    "                exp_value = ( 1 - epsilon + epsilon / n_actions) * self.q[tuple([self.x, self.y, next_action])]\n",
    "                + sum( (( epsilon / n_actions ) * self.q[tuple([self.x, self.y, a])]) for a in actions)\n",
    "                self.q[sa] = self.q[sa] + alpha * ( reward + self.gamma * self.q[tuple([self.x, self.y, next_action])] - self.q[sa])\n",
    "                if self.terminated():\n",
    "                    episode_count+=1\n",
    "                    axis_timesteps.append(step_count)\n",
    "                    axis_episodic.append(episode_count)\n",
    "                    break\n",
    "                else:\n",
    "                    axis_timesteps.append(step_count)\n",
    "                    axis_episodic.append(episode_count)\n",
    "                    \n",
    "        return axis_timesteps, axis_episodic\n",
    "    \n",
    "    # n-step SARSA with n = 4\n",
    "    def nstepSARSA(self, max_steps=8000, n=4):\n",
    "        axis_timesteps, axis_episodic = [], []\n",
    "        episode_count = 0\n",
    "        step_count = 0\n",
    "        lr = 0.5\n",
    "        epsilon = 0.1\n",
    "        finished = False\n",
    "        while not finished:\n",
    "            self.reset()\n",
    "            t = 0\n",
    "            T = np.inf\n",
    "            action = self.p[tuple([self.x, self.y])]\n",
    "            actions = [action]\n",
    "            states = [tuple([self.x, self.y])]\n",
    "            rewards = [0]\n",
    "            while not finished:\n",
    "                if not step_count < max_steps:\n",
    "                    finished = True\n",
    "                    break\n",
    "                if t < T:\n",
    "                    reward = self.step(action)\n",
    "                    state = tuple([self.x, self.y])\n",
    "\n",
    "                    states.append(state)\n",
    "                    rewards.append(reward)\n",
    "\n",
    "                    if self.terminated():\n",
    "                        episode_count+=1\n",
    "                        T = t + 1\n",
    "                    else:\n",
    "                        if np.random.choice([True, False], p=[epsilon, 1-epsilon]):\n",
    "                            action = random.choice(list(action_map.keys()))\n",
    "                        else:\n",
    "                            action = max(action_map.keys(), key = lambda a: self.q[tuple([self.x, self.y, a])])\n",
    "                        actions.append(action)  \n",
    "                tau = t - n + 1\n",
    "                step_count+=1\n",
    "                axis_timesteps.append(step_count)\n",
    "                axis_episodic.append(episode_count)\n",
    "                if tau >= 0:\n",
    "                    G = 0\n",
    "                    for i in range(tau + 1, min(tau + n + 1, T + 1)):\n",
    "                        G += np.power(self.gamma, i - tau - 1) * rewards[i]\n",
    "                    if tau + n < T:\n",
    "                        state_action = (states[tau + n], actions[tau + n])\n",
    "                        state_action = [*state_action[0]] + [state_action[1]]\n",
    "                        G += np.power(self.gamma, n) * self.q[tuple(state_action)]\n",
    "                    state_action = (states[tau], actions[tau])\n",
    "                    state_action = [*state_action[0]] + [state_action[1]]\n",
    "                    self.q[tuple(state_action)] += lr * (\n",
    "                                G - self.q[tuple(state_action)])\n",
    "                if tau == T - 1:\n",
    "                    break\n",
    "\n",
    "                t += 1\n",
    "        return list(range(8000)), axis_episodic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fossil-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(trials, function):\n",
    "    x_list, y_list = [], []\n",
    "    for run in range(trials):\n",
    "        agent = Agent()\n",
    "        \n",
    "        function_map = {\n",
    "            \"MC\": agent.MC,\n",
    "            \"SARSA\": agent.SARSA,\n",
    "            \"qLearning\": agent.qLearning,\n",
    "            \"expSARSA\": agent.expSARSA,\n",
    "            \"nstepSARSA\": agent.nstepSARSA\n",
    "        }\n",
    "        \n",
    "        x, y = function_map[function]()\n",
    "        x_list.append(np.array(x))\n",
    "        y_list.append(np.array(y))\n",
    "        \n",
    "    return list(range(8000)), np.array(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sexual-judgment",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_clip_dispatcher() missing 2 required positional arguments: 'a_min' and 'a_max'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ca24d2daa7d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# MC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"MC\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'MC'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrew_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.96\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-24fc33bfc364>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(trials, function)\u001b[0m\n\u001b[1;32m     12\u001b[0m         }\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mx_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0my_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e224fa0607e4>\u001b[0m in \u001b[0;36mMC\u001b[0;34m(self, max_steps, max_moves, epsilon)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0msteps_count\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mepisode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_prev\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e224fa0607e4>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgridworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgridworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mwind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-60747a2d59c3>\u001b[0m in \u001b[0;36mreward\u001b[0;34m(self, x, y, wind)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mwind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# Input x, y --> Output True if position is terminal state else False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mclip\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: _clip_dispatcher() missing 2 required positional arguments: 'a_min' and 'a_max'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1280x640 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting code\n",
    "plt.figure(figsize=(16, 8), dpi=80)\n",
    "\n",
    "# MC\n",
    "x, y = test(20,\"MC\")\n",
    "plt.plot(x, np.mean(y, axis=0), color='red', label='MC')\n",
    "rew_err = 1.96 * (np.std(np.mean(y, axis=0), axis=0) / np.sqrt(range(1,8001)))\n",
    "plt.fill_between(range(8000), np.mean(y, axis=0) - rew_err, np.mean(y, axis=0) + rew_err, alpha=0.2, color='red')\n",
    "\n",
    "# SARSA\n",
    "x, y = test(20,\"SARSA\")\n",
    "plt.plot(x, np.mean(y, axis=0), color='blue', label='SARSA')\n",
    "rew_err = 1.96 * (np.std(np.mean(y, axis=0), axis=0) / np.sqrt(range(1,8001)))\n",
    "plt.fill_between(range(8000), np.mean(y, axis=0) - rew_err, np.mean(y, axis=0) + rew_err, alpha=0.2, color='blue')\n",
    "\n",
    "# qLearning\n",
    "x, y = test(20,\"qLearning\")\n",
    "plt.plot(x, np.mean(y, axis=0), color='green', label='qLearning')\n",
    "rew_err = 1.96 * (np.std(np.mean(y, axis=0), axis=0) / np.sqrt(range(1,8001)))\n",
    "plt.fill_between(range(8000), np.mean(y, axis=0) - rew_err, np.mean(y, axis=0) + rew_err, alpha=0.2, color='green')\n",
    "\n",
    "# expSARSA\n",
    "x, y = test(20,\"expSARSA\")\n",
    "plt.plot(x, np.mean(y, axis=0), color='orange', label='expSARSA')\n",
    "rew_err = 1.96 * (np.std(np.mean(y, axis=0), axis=0) / np.sqrt(range(1,8001)))\n",
    "plt.fill_between(range(8000), np.mean(y, axis=0) - rew_err, np.mean(y, axis=0) + rew_err, alpha=0.2, color='orange')\n",
    "\n",
    "# nstepSARSA\n",
    "x, y = test(20,\"nstepSARSA\")\n",
    "plt.plot(x, np.mean(y, axis=0), color='brown', label='nstepSARSA')\n",
    "rew_err = 1.96 * (np.std(np.mean(y, axis=0), axis=0) / np.sqrt(range(1,8001)))\n",
    "plt.fill_between(range(8000), np.mean(y, axis=0) - rew_err, np.mean(y, axis=0) + rew_err, alpha=0.2, color='brown')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-attention",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-morris",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
